Authors,Date of Publication,Title,Most Important Findings,Methods
"Paul F. Simmering, Paavo Huoviala","October 30, 2023",Large Language Models for Aspect-Based Sentiment Analysis,"- GPT-3.5 achieved a state-of-the-art F1 score of 83.8 on the joint aspect term extraction and polarity classification task, outperforming the previous state-of-the-art model by 5.7%. - Fine-tuned models showed superior performance in aspect-based sentiment analysis compared to non-fine-tuned models. - Detailed prompts improved performance in zero-shot and few-shot settings but were not necessary for fine-tuned models. - Fine-tuned GPT-3.5 models were more sensitive and specific, making fewer errors compared to GPT-3.5 and GPT-4. - The choice of prompt and fine-tuning significantly influenced model performance and cost-efficiency.","- Evaluation was done on the SemEval-2014 dataset for aspect-based sentiment analysis. - Performance was measured using F1 score on the joint aspect term extraction and polarity classification task. - Various prompt variations were tested, along with in-context examples and fine-tuning using OpenAI’s API. - Error analysis was conducted to understand the models' strengths and weaknesses. - An economic analysis was performed to assess the cost-effectiveness of different model versions and conditions."
"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",2nd August 2023,Attention Is All You Need,"- Proposed the Transformer model based solely on attention mechanisms, outperforming existing models in machine translation tasks. - Achieved superior quality in translation, more parallelizable, and faster to train compared to traditional models. - Transformer achieved a BLEU score of 28.4 on the WMT 2014 English-to-German task and 41.8 on the English-to-French task. - Generalized well to other tasks like English constituency parsing, surpassing previous models in accuracy.","- Introduced the Transformer architecture based on attention mechanisms, eliminating the need for recurrence and convolutions. - Used self-attention, multi-head attention, and position-wise feed-forward networks in both encoder and decoder stacks. - Employed techniques like scaled dot-product attention, positional encoding, and label smoothing during training. - Trained models on the WMT 2014 English-German dataset using the Adam optimizer with varying learning rates. - Conducted experiments on English constituency parsing to demonstrate the model's generalization to other tasks. - Shared code used for training and evaluating the models."
